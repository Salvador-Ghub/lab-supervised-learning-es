{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Índice de contenidos\n",
    "1. Antes de empezar\n",
    "\n",
    "2. Reto 1 - Explorar el conjunto de datos\n",
    "\n",
    "    2.0.0.1 Explore los datos a vista de pájaro.\n",
    "    \n",
    "    2.0.0.2 A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "3. Reto 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "4. Reto 3 - Manejar los valores perdidos\n",
    "\n",
    "    4.0.0.1 En las celdas siguientes, trate los valores que faltan en el conjunto de datos. Recuerde comentar los fundamentos de sus decisiones.\n",
    "    \n",
    "    4.0.0.2 De nuevo, examine el número de valores que faltan en cada columna.\n",
    "\n",
    "5. Reto 4 - Manejo de datos categóricos WHOIS_*\n",
    "    \n",
    "    5.0.0.1 En las celdas siguientes, fije los valores de los países como se ha indicado anteriormente.\n",
    "    \n",
    "    5.0.0.2 Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "    \n",
    "    5.0.0.3 Después de comprobarlo, mantengamos los 10 valores principales de la columna y reetiquetemos las demás columnas con OTROS.\n",
    "    \n",
    "    5.0.0.4 En la siguiente celda, elimine ['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'].\n",
    "\n",
    "6. Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "    \n",
    "    6.0.0.1 URL es fácil. Simplemente la eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar.\n",
    "    \n",
    "    6.0.0.2 Imprima el conteo de valores únicos de CHARSET. Puede ver que sólo hay unos pocos valores únicos. Así que podemos dejarlo como está.\n",
    "    \n",
    "    6.0.0.3 Antes de pensar en su propia solución, no lea las instrucciones que vienen a continuación.\n",
    "\n",
    "7. Desafío 6 - Modelado, predicción y evaluación\n",
    "    \n",
    "    7.0.0.1 En este laboratorio probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "    \n",
    "    7.0.0.2 Nuestro segundo algoritmo es DecisionTreeClassifier.\n",
    "    \n",
    "    7.0.0.3 Crearemos otro modelo DecisionTreeClassifier con max_depth=5.\n",
    "\n",
    "8. Bonus Challenge - Escalado de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar:\n",
    "- Lee el archivo README.md\n",
    "- Comenta todo lo que puedas y utiliza los recursos del archivo README.md\n",
    "- ¡Feliz aprendizaje!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your libraries:\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este laboratorio, exploraremos un conjunto de datos que describe sitios web con diferentes características y los etiqueta como benignos o maliciosos. Utilizaremos algoritmos de aprendizaje supervisado para averiguar qué patrones de características es probable que tengan los sitios web maliciosos y utilizaremos nuestro modelo para predecir sitios web maliciosos.\n",
    "\n",
    "Sus características serán:\n",
    "\n",
    "+ URL: es la identificación anónima de la URL analizada en el estudio\n",
    "+ URL_LENGTH: es el número de caracteres de la URL\n",
    "+ NUMBER_SPECIAL_CHARACTERS: es el número de caracteres especiales identificados en la URL, como, «/», «%», «#», «&», «. “, ”=»\n",
    "+ CHARSET: es un valor categórico y su significado es el estándar de codificación de caracteres (también llamado juego de caracteres).\n",
    "+ SERVER: es un valor categórico y su significado es el sistema operativo del servidor obtenido de la respuesta del paquete.\n",
    "+ CONTENT_LENGTH: representa el tamaño del contenido de la cabecera HTTP.\n",
    "+ WHOIS_COUNTRY: es una variable categórica, sus valores son los países que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_STATEPRO: es una variable categórica, sus valores son los estados que obtuvimos de la respuesta del servidor (en concreto, nuestro script utilizó la API de Whois).\n",
    "+ WHOIS_REGDATE: Whois proporciona la fecha de registro del servidor, por tanto, esta variable tiene valores de fecha con formato DD/MM/AAAA HH:MM\n",
    "+ WHOIS_UPDATED_DATE: A través del Whois obtenemos la última fecha de actualización del servidor analizado\n",
    "+ TCP_CONVERSATION_EXCHANGE: Esta variable es el número de paquetes TCP intercambiados entre el servidor y nuestro cliente honeypot\n",
    "+ DIST_REMOTE_TCP_PORT: es el número de puertos detectados y diferentes a TCP\n",
    "+ REMOTE_IPS: esta variable tiene el número total de IPs conectadas al honeypot\n",
    "+ APP_BYTES: es el número de bytes transferidos\n",
    "+ SOURCE_APP_PACKETS: paquetes enviados desde el honeypot al servidor\n",
    "+ REMOTE_APP_PACKETS: paquetes recibidos del servidor\n",
    "+ APP_PACKETS: número total de paquetes IP generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ DNS_QUERY_TIMES: número de paquetes DNS generados durante la comunicación entre el honeypot y el servidor.\n",
    "+ TYPE: es una variable categórica, sus valores representan el tipo de página web analizada, en concreto, 1 es para sitios web maliciosos y 0 para sitios web benignos\n",
    "\n",
    "# Desafío 1 - Explorar el conjunto de datos\n",
    "\n",
    "Empecemos explorando el conjunto de datos. Primero carga el archivo de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = pd.read_csv('../website.csv')\n",
    "df = websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore los datos a vista de pájaro.\n",
    "\n",
    "Ahora ya deberías estar muy familiarizado con los procedimientos, así que no te daremos las instrucciones paso a paso. Reflexiona sobre lo que hiciste en los laboratorios anteriores y explora el conjunto de datos.\n",
    "\n",
    "Cosas que buscarás:\n",
    "\n",
    "* ¿Qué aspecto tiene el conjunto de datos?\n",
    "* ¿Cuáles son los tipos de datos?\n",
    "* ¿Qué columnas contienen las características de los sitios web?\n",
    "* ¿Qué columna contiene la característica que vamos a predecir? ¿Cuál es el código de los sitios web benignos frente a los maliciosos?\n",
    "* ¿Necesitamos transformar alguna de las columnas de categórica a ordinal? En caso afirmativo, ¿cuáles son esas columnas?\n",
    "\n",
    "Siéntete libre de añadir celdas adicionales para tus exploraciones. Asegúrate de comentar lo que descubras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the data types?\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = df\n",
    "# Análisis de calidad de la base de datos\n",
    "print(\"TOTAL NUMBER OF ROWS: \",len(x),\"\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\")\n",
    "print(\"NUMBER OF UNIQUE VALUES:\",x.nunique(),\"\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "print(\"NUMBER OF UNIQUE VALUES DIVIDED BY TOTAL VALUES:\",round((x.nunique()/len(x)*100),2),\"%\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "print(\"% OF NAN VALUES IS: \",round(100*(x.isnull().sum() / len(x)),2),\"%\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which columns contain the features of the websites?\n",
    "Todos excepto Type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which column contains the feature we will predict? What is the code standing for benign vs malicious websites?\n",
    "Type\n",
    "binaria: 1 es para sitios web maliciosos y 0 para sitios web benignos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.URL.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do we need to transform any of the columns from categorical to ordinal values? If so what are these columns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir todas las cadenas a minúsculas para normalizar\n",
    "df['WHOIS_STATEPRO_NORM'] = df['WHOIS_STATEPRO'].str.lower()\n",
    "\n",
    "df.WHOIS_STATEPRO_NORM.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ordenar por longitud de la cadena para evitar eliminar cadenas completas antes de verificar subcadenas\n",
    "# df = df.sort_values(by='WHOIS_STATEPRO_NORM', key=lambda col: col.str.len()).reset_index(drop=True)\n",
    "\n",
    "# # Quitar duplicados basados en subcadenas\n",
    "# def eliminar_subcadenas(data):\n",
    "#     result = []\n",
    "#     for i, texto in enumerate(data):\n",
    "#         # Solo agregar si no está contenido en ningún texto ya procesado\n",
    "#         if not any(texto in t for t in result):\n",
    "#             result.append(texto)\n",
    "#     return result\n",
    "\n",
    "# # Aplicar la función para quitar duplicados \"escondidos\"\n",
    "# df_limpio = pd.DataFrame({'texto': eliminar_subcadenas(df['WHOIS_STATEPRO_NORM'])})\n",
    "\n",
    "# # Resultado\n",
    "# print(df_limpio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A continuación, evalúe si las columnas de este conjunto de datos están fuertemente correlacionadas.\n",
    "\n",
    "En el laboratorio de aprendizaje supervisado Mushroom que hicimos recientemente, mencionamos que nos preocupa si nuestro conjunto de datos tiene columnas fuertemente correlacionadas porque si es el caso tenemos que elegir ciertos algoritmos de ML en lugar de otros. Ahora tenemos que evaluar esto para nuestro conjunto de datos.\n",
    "\n",
    "Por suerte, la mayoría de las columnas de este conjunto de datos son ordinales, lo que nos facilita mucho las cosas. En las siguientes celdas, evalúe el nivel de colinealidad de los datos.\n",
    "\n",
    "Aquí tienes algunas indicaciones generales que puede consultar para completar este paso:\n",
    "\n",
    "1. Crea una matriz de correlaciones utilizando las columnas numéricas del conjunto de datos.\n",
    "\n",
    "2. Crea un mapa de calor utilizando `seaborn` para visualizar qué columnas tienen una alta colinealidad.\n",
    "\n",
    "3. Comenta qué columnas podría necesitar eliminar debido a la alta colinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df_num = df.select_dtypes(include='number')\n",
    "correlation_matrix = df_num.corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', square=True, cmap='viridis')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna de Source app packets por su correlación tan alta con remote app rackets\n",
    "df = df.drop(columns=['SOURCE_APP_PACKETS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['REMOTE_APP_BYTES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['APP_PACKETS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['REMOTE_APP_PACKETS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['NUMBER_SPECIAL_CHARACTERS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este es un ejemplo para conocer la importancia de las características usando un modelo ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "random_state = 42\n",
    "xgb = xgb.XGBClassifier(random_state=random_state)\n",
    "X = websites._get_numeric_data().drop('Type', axis=1)\n",
    "y = websites.Type\n",
    "xgb.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = xgb.feature_importances_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(X.columns[sort_idx],xgb.feature_importances_[sort_idx])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico anterior podemos ver las características con menor peso en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 2 - Eliminar la colinealidad de columnas.\n",
    "\n",
    "En el mapa de calor que ha creado, deberías haber visto al menos 3 columnas que pueden eliminarse debido a la alta colinealidad. Elimina estas columnas del conjunto de datos.\n",
    "\n",
    "Ten en cuenta que debes eliminar el menor número posible de columnas. No tienes que eliminar todas las columnas a la vez. En su lugar, intenta eliminar una columna y, a continuación, vuelve a elaborar el mapa térmico para determinar si deben eliminarse columnas adicionales. Cuando el conjunto de datos ya no contenga columnas correlacionadas en más de un 90%, puedes parar. Además, ten en cuenta que cuando dos columnas tienen una alta colinealidad, sólo necesitas eliminar una de ellas, pero no ambas.\n",
    "\n",
    "En las celdas de abajo, elimina tantas columnas como puedas para eliminar la alta colinealidad en el conjunto de datos. Asegúrate de comentar tu camino para que se pueda conocer tu razonamiento, lo que permitirá dar feedback. Al final, vuelve a imprimir el mapa de calor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# ya realizado previamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE THE 4 COLUMNS WITH MORE COLLINEARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 3 - Manejar los valores que faltan\n",
    "\n",
    "El siguiente paso sería manejar los valores faltantes. **Comenzamos examinando el número de valores que faltan en cada columna.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will drop the columns with more than 50% of missing data\n",
    "df = df.drop(columns=['CONTENT_LENGTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will drop the rows with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De nuevo, examina el número de valores que faltan en cada columna. \n",
    "\n",
    "    Si todos están limpios, procede. Si no, vuelve atrás y haz más limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine missing values in each column\n",
    "x = df\n",
    "# Análisis de calidad de la base de datos\n",
    "print(\"TOTAL NUMBER OF ROWS: \",len(x),\"\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\")\n",
    "print(\"NUMBER OF UNIQUE VALUES:\",x.nunique(),\"\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "print(\"NUMBER OF UNIQUE VALUES DIVIDED BY TOTAL VALUES:\",round((x.nunique()/len(x)*100),2),\"%\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")\n",
    "print(\"% OF NAN VALUES IS: \",round(100*(x.isnull().sum() / len(x)),2),\"%\\n\")\n",
    "print(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificamos que es recomendable rellenar los valores faltantes mediante un fillna, aplicando la media o modas para completarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 4 - Manejar datos categóricos `WHOIS_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias columnas categóricas que necesitamos manejar. Estas columnas son:\n",
    "\n",
    "* URL\n",
    "* CHARSET\n",
    "* SERVIDOR\n",
    "* PAÍS\n",
    "* «WHOIS_STATEPRO\n",
    "* WHOIS_REGDATE\n",
    "* WHOIS_UPDATED_DATE\n",
    "\n",
    "La forma de tratar las columnas de cadena es siempre caso por caso. Empecemos trabajando con `WHOIS_COUNTRY`. Tus pasos son:\n",
    "\n",
    "1. Enumera los valores únicos de `WHOIS_COUNTRY`.\n",
    "1. Consolide los valores de país con códigos de país coherentes. Por ejemplo, los siguientes valores se refieren al mismo país y deben utilizar un código de país coherente:\n",
    "    * `CY` y `Cyprus`.\n",
    "    * US y US\n",
    "    * SE y SE\n",
    "    * GB, Reino Unido y GB, Reino Unido.\n",
    "\n",
    "#### En las celdas de abajo, fija los valores de los países como se indica arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "good_country = {'None':'None', \n",
    "                'US':'US', \n",
    "                'SC':'SC', \n",
    "                'GB':'UK', \n",
    "                'UK':'UK', \n",
    "                'RU':'RU', \n",
    "                'AU':'AU', \n",
    "                'CA':'CA',\n",
    "                'PA':'PA',\n",
    "                'se':'SE', \n",
    "                'IN':'IN',\n",
    "                'LU':'LU', \n",
    "                'TH':'TH', \n",
    "                \"[u'GB'; u'UK']\":'UK', \n",
    "                'FR':'FR',\n",
    "                'NL':'NL',\n",
    "                'UG':'UG', \n",
    "                'JP':'JP', \n",
    "                'CN':'CN', \n",
    "                'SE':'SE',\n",
    "                'SI':'SI', \n",
    "                'IL':'IL', \n",
    "                'ru':'RU', \n",
    "                'KY':'KY', \n",
    "                'AT':'AT', \n",
    "                'CZ':'CZ', \n",
    "                'PH':'PH', \n",
    "                'BE':'BE', \n",
    "                'NO':'NO', \n",
    "                'TR':'TR', \n",
    "                'LV':'LV',\n",
    "                'DE':'DE', \n",
    "                'ES':'ES', \n",
    "                'BR':'BR', \n",
    "                'us':'US', \n",
    "                'KR':'KR', \n",
    "                'HK':'HK', \n",
    "                'UA':'UA', \n",
    "                'CH':'CH', \n",
    "                'United Kingdom':'UK',\n",
    "                'BS':'BS', \n",
    "                'PK':'PK', \n",
    "                'IT':'IT', \n",
    "                'Cyprus':'CY', \n",
    "                'BY':'BY', \n",
    "                'AE':'AE', \n",
    "                'IE':'IE', \n",
    "                'UY':'UY', \n",
    "                'KG':'KG'}\n",
    "\n",
    "df.WHOIS_COUNTRY = df.WHOIS_COUNTRY.apply(lambda x : good_country[x])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que hemos fijado los valores de los países, ¿podemos convertir ahora esta columna en ordinal?\n",
    "\n",
    "Todavía no. Si reflexionas, en los laboratorios anteriores sobre cómo manejamos las columnas categóricas, probablemente recuerdes que acabamos eliminando muchas de esas columnas porque hay demasiados valores únicos. Demasiados valores únicos en una columna no es deseable en el aprendizaje automático porque hace que la predicción sea inexacta. Pero hay soluciones bajo ciertas condiciones. Una de las condiciones solucionables es:\n",
    "\n",
    "#### Si un número limitado de valores representa la mayoría de los datos, podemos conservar estos valores principales y volver a etiquetar todos los demás valores poco frecuentes.\n",
    "\n",
    "La columna `WHOIS_COUNTRY` resulta ser este caso. Puedes comprobarlo imprimiendo un gráfico de barras de los `value_counts` en la siguiente celda para verificarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def print_bar_plot(x,y):\n",
    "    plt.bar(x, y)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WHOIS_COUNTRY.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.WHOIS_COUNTRY.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_barh_plot(x,y):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.barh(x, y)\n",
    "    return plt.show()\n",
    "\n",
    "print_barh_plot(df.WHOIS_COUNTRY.value_counts(ascending=True).index,df.WHOIS_COUNTRY.value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Después de verificar, ahora vamos a mantener los 10 primeros valores de la columna y volver a etiquetar otras columnas con `OTHER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = df.WHOIS_COUNTRY.value_counts().head(10)\n",
    "print(top10.iloc[0])\n",
    "top10_pais = top10.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "df.WHOIS_COUNTRY.value_counts().head(10)\n",
    "\n",
    "df['WHOIS_COUNTRY'] = df['WHOIS_COUNTRY'].apply(lambda x: x if x in top10_pais else 'Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que se ha cambiado la etiqueta `WHOIS_COUNTRY`, ya no necesitamos `WHOIS_STATEPRO` porque los valores de los estados o provincias pueden dejar de ser relevantes. Eliminaremos esta columna.\n",
    "\n",
    "Además, también eliminaremos `WHOIS_REGDATE` y `WHOIS_UPDATED_DATE`. Se trata de las fechas de registro y actualización de los dominios del sitio web. No son de nuestra incumbencia.\n",
    "\n",
    "#### En la siguiente celda, elimina `['WHOIS_STATEPRO', 'WHOIS_REGDATE', 'WHOIS_UPDATED_DATE']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.drop(columns = 'WHOIS_STATEPRO')\n",
    "df = df.drop(columns = 'WHOIS_REGDATE')\n",
    "df = df.drop(columns = 'WHOIS_UPDATED_DATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reto 5 - Manejar los datos categóricos restantes y convertirlos en ordinales\n",
    "\n",
    "Ahora vuelve a imprimir los `dtypes` de los datos. Además de `WHOIS_COUNTRY` que ya hemos arreglado, deberían quedar 3 columnas categóricas: `URL`, `CHARSET`, y `SERVER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "print(df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `URL` es fácil. Simplemente lo eliminaremos porque tiene demasiados valores únicos que no hay forma de consolidar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df = df.drop(columns = 'URL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imprime el recuento de valores únicos de `CHARSET`. Usted ve que hay sólo unos pocos valores únicos. Así que podemos dejarlo como está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "df['CHARSET'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SERVER'].nunique()\n",
    "df['SERVER'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SERVER` es un poco más complicado. Imprime sus valores únicos y piensa cómo puedes consolidar esos valores.\n",
    "\n",
    "#### Antes de pensar en tu propia solución, no leas las instrucciones que vienen a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "reemplazos = {\n",
    "    \"microsoft\": \"Microsoft\",\n",
    "    \"apache\": \"Apache\",\n",
    "    \"nginx\": \"nginx\"\n",
    "}\n",
    "\n",
    "# Reemplazar los textos en la columna \"Texto\"\n",
    "def reemplazar_texto(texto, reemplazos):\n",
    "    for palabra, nuevo_texto in reemplazos.items():\n",
    "        if palabra in texto:\n",
    "            return nuevo_texto\n",
    "    return 'Other'  # Si no encuentra ninguna palabra, deja el texto original\n",
    "\n",
    "df[\"SERVER\"] = df[\"SERVER\"].apply(lambda x: reemplazar_texto(x.lower(), reemplazos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque hay tantos valores únicos en la columna `SERVER`, en realidad sólo hay 3 tipos principales de servidores: Microsoft, Apache y Nginx. Simplemente comprueba si cada valor de `SERVER` contiene alguno de esos tipos de servidor y vuelve a etiquetarlos. Para los valores `SERVER` que no contengan ninguna de esas subcadenas, etiquétalos con `Other`.\n",
    "\n",
    "Al final, la columna «SERVIDOR» sólo debe contener 4 valores únicos: `Microsoft`, `Apache`, `nginx`, y `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count `SERVER` value counts here\n",
    "df['SERVER'].nunique()\n",
    "#df['SERVER'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, todos nuestros datos categóricos están fijados ahora. **Vamos a convertirlos en datos ordinales usando la función `get_dummies` de Pandas ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)). Asegúrate de eliminar las columnas categóricas pasando `drop_first=True` a `get_dummies` ya que no las necesitamos. **Además, asigna los datos con valores ficticios a una nueva variable `website_dummy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "df = df.drop(columns = 'WHOIS_STATEPRO_NORM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, inspeccione `website_dummy` para asegurarse de que los datos y tipos son los previstos - no debería haber ninguna columna categórica en este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "website_dummy.head()\n",
    "website_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafío 6 - Modelado, predicción y evaluación\n",
    "\n",
    "Comenzaremos esta sección dividiendo los datos en train y test. **Nombra tus 4 variables `X_entrenamiento`, `X_prueba`, `y_entrenamiento` y `y_prueba`. Selecciona el 80% de los datos para entrenar y el 20% para probar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = website_dummy.drop(columns = [\"Type\"])\n",
    "target = website_dummy[\"Type\"]\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este laboratorio, probaremos dos modelos diferentes y compararemos nuestros resultados.\n",
    "\n",
    "El primer modelo que utilizaremos en este laboratorio es la regresión logística. Ya hemos aprendido sobre la regresión logística como algoritmo de clasificación. En la celda de abajo, cargue `LogisticRegression` de scikit-learn e inicialice el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Crear el modelo de regresión logística\n",
    "modelo = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Entrenar el modelo\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "y_pred = modelo.predict(X_test)\n",
    "\n",
    "# Evaluar el modelo\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nReporte de Clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Matriz de Confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Mostrar las probabilidades predichas\n",
    "probabilidades = modelo.predict_proba(X_test)\n",
    "print(\"\\nProbabilidades de las clases (0, 1):\\n\", probabilidades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, ajustamos el modelo a nuestros datos de entrenamiento. Ya hemos separado nuestros datos en 4 partes. Utilízalos en tu modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, importamos `confusion_matrix` y `accuracy_score` de `sklearn.metrics` y ajustamos nuestros datos de prueba. Asigna los datos ajustados a `y_pred` e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué opinas del rendimiento del modelo? Escribe tus conclusiones a continuación.\n",
    "\n",
    "La precisión es muy correcta, después de tanto preprocesado los datos están suficientemente limpios como para resultar en predicciones cercanas a la realidad. Generaliza bastante bien. Aunque podríamos normalizar los datos para eliminar outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuestro segundo algoritmo es DecisionTreeClassifier\n",
    "\n",
    "Aunque no es necesario, vamos a ajustar un modelo utilizando los datos de entrenamiento y luego probar el rendimiento del modelo utilizando los datos de prueba. Empezaremos cargando `DecisionTreeClassifier` de scikit-learn y luego inicializando y ajustando el modelo. Empezaremos con un modelo donde max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "iris_names=['1', '0']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True, feature_names=features.columns, class_names=iris_names)\n",
    "plt.show()\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Obtener la matriz de confusión\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "#Crear un mapa de calor para visualizar la matriz de confusión\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "#Agregar etiquetas y títulos\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar tu modelo, calcula las probabilidades predichas, decide 0 o 1 utilizando un umbral de 0,5 e imprime la matriz de confusión, así como la puntuación de precisión (en el conjunto de prueba)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos a crear otro modelo DecisionTreeClassifier con max_depth=5. \n",
    "Inicia y ajusta el modelo de abajo e imprime la matriz de confusión y la puntuación de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "model = DecisionTreeClassifier(max_depth=5)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "iris_names=['1', '0']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True, feature_names=features.columns, class_names=iris_names)\n",
    "plt.show()\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "pred\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Obtener la matriz de confusión\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "\n",
    "#Crear un mapa de calor para visualizar la matriz de confusión\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "#Agregar etiquetas y títulos\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "#Mostrar la matriz de confusión\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Has observado una mejora en la matriz de confusión al aumentar max_depth a 5? ¿Has observado una mejora en la puntuación de precisión? Escribe tus conclusiones a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión:\n",
    "\n",
    "En mi caso no hay mejoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Add your conclusion here -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Escalado de características\n",
    "\n",
    "La resolución de problemas en el aprendizaje automático es iterativa. Puede mejorar la predicción de su modelo con diversas técnicas (aunque hay un punto óptimo para el tiempo que invierte y la mejora que obtiene). Ahora sólo has completado una iteración del análisis ML. Hay más iteraciones que puedes realizar para introducir mejoras. Para poder hacerlo, necesitarás conocimientos más profundos en estadística y dominar más técnicas de análisis de datos. En este bootcamp, no tenemos tiempo para alcanzar ese objetivo avanzado. Pero harás esfuerzos constantes después del bootcamp para conseguirlo finalmente.\n",
    "\n",
    "Sin embargo, ahora sí queremos que aprendas una de las técnicas avanzadas que se llama *feature scaling*. La idea del escalado de características es estandarizar/normalizar el rango de variables independientes o características de los datos. Esto puede hacer que los valores atípicos sean más evidentes para que pueda eliminarlos. Este paso debe realizarse durante el Desafío 6 después de dividir los datos de entrenamiento y de prueba, ya que no desea dividir los datos de nuevo, lo que hace imposible comparar los resultados con y sin el escalado de características. Para conceptos generales sobre el escalado de características, haga clic [aquí](https://en.wikipedia.org/wiki/Feature_scaling). Para profundizar, haga clic [aquí](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n",
    "\n",
    "En la siguiente celda, intente mejorar la precisión de predicción de su modelo mediante el escalado de características. Una librería que puedes utilizar es `sklearn.preprocessing.RobustScaler` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)). Utilizarás `RobustScaler` para ajustar y transformar tu `X_train`, y luego transformar `X_test`. Utilizarás la regresión logística para ajustar y predecir tus datos transformados y obtener la puntuación de precisión de la misma manera. Compare la puntuación de precisión con sus datos normalizados con los datos de precisión anteriores. ¿Se ha producido alguna mejora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "columnas_a_escalar = website_dummy.columns\n",
    "website_dummy[columnas_a_escalar] = scaler.fit_transform(website_dummy[columnas_a_escalar])\n",
    "website_dummy\n",
    "features = website_dummy.drop(columns = [\"Type\"])\n",
    "target = website_dummy[\"Type\"]\n",
    "\n",
    "# Your code here:\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your comments here: En las pruebas realizadas el resultado ha sido exactamente el mismo. Pero entiendo que esto ayuda para eliminar outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
